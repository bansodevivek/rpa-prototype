# main.py

from automation.login import login
from automation.scraper import scrape_quotes
from database.db_handler import init_db, insert_quotes

def main():
    print("ğŸŸ¢ main.py starting...")

    # Step 1: Initialize the database and create table if needed
    init_db()

    # Step 2: Login to the website using Playwright automation
    try:
        print("ğŸš€ Launching browser and logging in...")
        playwright, browser, page = login()
    except Exception as e:
        print(f"âŒ Error during login: {e}")
        return

    # Step 3: Confirm login success by printing page title
    try:
        title = page.title()
        print(f"ğŸ“„ Page Title After Login: {title}")
    except Exception as e:
        print(f"âš ï¸ Could not get page title: {e}")

    # Step 4: Scrape quotes from the page
    try:
        print("ğŸ” Scraping quotes...")
        quotes = scrape_quotes(page)
        print(f"âœ… Scraped {len(quotes)} quotes.")
    except Exception as e:
        print(f"âŒ Error during scraping: {e}")
        quotes = []

    # Step 5: Insert scraped quotes into the SQLite database
    try:
        insert_quotes(quotes)
        print(f"âœ… {len(quotes)} quotes saved to database.")
    except Exception as e:
        print(f"âŒ Error inserting quotes into database: {e}")

    # Step 6: Pause to keep the browser open for inspection
    input("â¸ï¸ Press Enter to close browser and exit...")

    # Step 7: Cleanup resources - close browser and stop playwright
    try:
        browser.close()
        playwright.stop()
        print("ğŸ‘‹ Browser closed and Playwright stopped.")
    except Exception as e:
        print(f"âš ï¸ Error during cleanup: {e}")

if __name__ == "__main__":
    main()
